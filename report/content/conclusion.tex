
%Restate
Here I implemented a deep neural network agent for mountain car and breakout. I used DQN with experience replay and infrequent weight updates. I tried different sizes for the replay buffer and looked at the influence of weight updates on the algorithm. %
%
%explain
The goal was to find out how the learning stability is influenced by tweaking these parts. Due to the lengthy training times for the breakout agent we did this exclusively for the mountain car problem. I tried halving and doubling the replay buffer size and turning off and on the infrequent weight updates enhancement. I ran each variant four times to account for inherent variation in the training.
%
%results
The mountain car agents learning was unstable by default, 1 out of 4 times it would not learn at all and one run it forgot almost immediately what it learned, only perfoming well for a few hunderd iterations. When the replay buffer was halved only the number of times the agent would not learn at all doubled though when it started to learn it would seem more stable. Doubling the replay buffer size eliminated the times the agent would not learn at all. The agent was more stable. I conclude that a larger replay buffer helps training stability a lot, however it might destabilizes training that is going well introducing bad results from the past. Taking the best result, the doubled replay buffer, and disabling infrequent weight updates had two effects. Learning destabilized dramatically, only 1 of 4 agents succeeded during training. The agent that did succeed learned dramatically faster which can be explained by the decrease in delay between learning and acting.

Moving on to the breakout agent. We see that training for one million steps did nothing for the performance of the agent, not beating a random player. When we train longer we see 2 large peaks in performance in the beginning followed by a number of smaller shorter ones before the agent dramatically and consistently improves its performance at iteration $36000$ about $75$\% through training. When we look closer we see the jump is as "instant" as it looked taking about $20$ games. The breakout agent is promising but at an average score of 12 after playing still below human levels of performance.
%
%uncertanties
Investing reinforcement learning agents is inherently unstable itself. As there is a lot of random decision making involved that could make or break an agent multiple training runs are needed to see if results can be replicated. Though we were easily able to replicate the results for the mountain car agent (as presented) I could not do multiple runs for the breakout agent as training it took over $2$ day and compute resources where limited\footnote{taking 4 computers for over 2 days to train multiple runs when access to free computers was deemed "not nice"}.
%
%new
It would be interesting to increase the size of the replay buffer for the breakout agent, support for this was added in the code and memory use could further be reduced by no longer storing the \texttt{after} action frames as the before after pairs can be recreated from the memory. A major flaw in the breakout agent is that it determines its next action on the before action frame. That means the agent has to think ahead one more frame than is necessary. This flaw was discovered when no more time was left for retraining the agent. The results above seem to indicate that infrequent weight updates somewhat inhibit learning, we could try disabling it when the agent is learning stabily. 