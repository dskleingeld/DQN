The field of reinforcement learning (RL) has long relied on heuristics, sampling and hand written feature recognition. To learn end to end, directly from high level input, has been a significant challenge. Utilizing (convolutional) neural networks the field of deep learning has succeeded to recognise features from high level input. Here we take a look at Deep Q networks (DQN) a successful approach to end-to-end reinforcement learning. The Q-table from classic Q-learning is replaced by a neural network where the training of the network mimics the Q-learning update formula.

There are a number of significant challenges. Reinforcement learning generates its own data, measures must be taken to prevent feedback loops. In deep learning the network immidiately gets feedback weather it was correct or not. In Reinforcement Learning there might be hunderds of not thousands of steps between an action and the reward. An RL agent must be able to deal with delayed rewards. 

Here we implement a DQN agent to solve the tasks mountain car and breakout. Mountain car is a simple control based task where one challange is that the reward is only given after the task is completed. Breakout is one of the classic Atari 2600 games sucessfully learned in the famouse "Human-level control through deep reinforcement learning"\cite{DQN} paper. I will use this to adapt the implementation for mountain car to work on breakout. I will use two of the learning techniques introduced there: experiance replay and infrequent weight updates. For the mountain car problem we will experiment with agents with various memory sizes for experiance replay and with and without infrequent weight updates. The breakout problem requires carefull tuning, together with lots of training, which did not leave time to experiment.

In the next section I discuss the theory of DQN, the challanges introduced above and how these are partially solved with experiance replay and infrequent weight updates. Then I will discuss my implementation of DQN for mountain car. This is followed by a section on the performance while experimenting with the agent. Next I shift the focus to breakout, detailing how the implementation for mountain car needed to be adjusted to get the breakout agent to learn. After this we take an in depth look at the training of the agent and how it performed. And in the final section of this report we conclude weather our agents have succeeded and how they could be improved. In the appendix at the end we detail how our agents can be tested.